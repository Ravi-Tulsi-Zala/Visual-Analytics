{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier : \n",
      "\n",
      "\t \t Accuracy of Logistic Regression Classifier  on training set: 83.22889426160755\n",
      "\t \t Accuracy of Logistic Regression Classifier  on validation set: 83.17025440313111 \n",
      "\n",
      "\t Trainset Statistics  : \n",
      "\n",
      "\t \t True Positives in train set  : 14052\n",
      "\t \t False Positives in train set : 1169\n",
      "\t \t False Negatives in train set : 2221\n",
      "\t \t True Negatives in train set  : 2825\n",
      "\t \t Average Class Accuracy in Train Set :  74.15237993236028\n",
      "\n",
      " \n",
      "\n",
      "\t Validation Set Statistics  : \n",
      "\n",
      "\t \t True Positives in validation set  : 6035\n",
      "\t \t False Positives in validation set : 506\n",
      "\t \t False Negatives in validation set : 956\n",
      "\t \t True Negatives in validation set  : 1190\n",
      "\t \t Average Class Accuracy in Validation Set :  73.85809175844444\n",
      "\n",
      " \n",
      "\n",
      "Decision Tree Classifier : \n",
      "\n",
      "\t \t Accuracy of Decision Tree Classifier on training set: 83.88019933882667\n",
      "\t \t Accuracy of Decision Tree Classifier on validation set: 84.06814780706803 \n",
      "\n",
      "\t Trainset Statistics  : \n",
      "\n",
      "\t \t True Positives in train set  : 14188\n",
      "\t \t False Positives in train set : 1033\n",
      "\t \t False Negatives in train set : 2725\n",
      "\t \t True Negatives in train set  : 2321\n",
      "\t \t Average Class Accuracy in Train Set :  69.60507643457213\n",
      "\n",
      " \n",
      "\n",
      "\t Validation Set Statistics  : \n",
      "\n",
      "\t \t True Positives in validation set  : 6216\n",
      "\t \t False Positives in validation set : 325\n",
      "\t \t False Negatives in validation set : 1059\n",
      "\t \t True Negatives in validation set  : 1087\n",
      "\t \t Average Class Accuracy in Validation Set :  72.84185864401374\n",
      "\n",
      " \n",
      "\n",
      "KNN Classifier : \n",
      "\n",
      "\t \t Accuracy of K-Nearest Neighbors classifier on training set: 88.82419697044458\n",
      "\t \t Accuracy of K-Nearest Neighbors classifier on validation set: 88.85691262806492 \n",
      "\n",
      "\t Trainset Statistics  : \n",
      "\n",
      "\t \t True Positives in train set  : 13359\n",
      "\t \t False Positives in train set : 1862\n",
      "\t \t False Negatives in train set : 2060\n",
      "\t \t True Negatives in train set  : 2986\n",
      "\t \t Average Class Accuracy in Train Set :  73.47124280676641\n",
      "\n",
      " \n",
      "\n",
      "\t Validation Set Statistics  : \n",
      "\n",
      "\t \t True Positives in validation set  : 6143\n",
      "\t \t False Positives in validation set : 398\n",
      "\t \t False Negatives in validation set : 570\n",
      "\t \t True Negatives in validation set  : 1576\n",
      "\t \t Average Class Accuracy in Validation Set :  83.67712983399713\n",
      "\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Hyperparameter Tuning in Decision Tree : \n",
      " \n",
      "\t \t Accuracy of Decision Tree Classifier on validation set using gini index: 85.26533901231726 \n",
      "\n",
      "\t \t Average Class Accuracy of Decision Tree Classifier on validation set using gini index: 75.23366127172883 \n",
      "\n",
      "12628\n"
     ]
    }
   ],
   "source": [
    "################################################ Libraries ######################################################################\n",
    "\n",
    "# Pandas package is useful for manipualting data using dataframes , numpy is useful for its powerful feature- arrays\n",
    "# pyplot calss of matplotlib is usefull for all the visualization stuffs like bar chart and line chart, sklearn contain wide \n",
    "# variety of classes and packages such as preprocessing, cross_validation, metrics, linear_model etc. It is also useful for \n",
    "# classification algorithms because of its extensive collection of algorithm classes like logisticRegression, DeicisionTreeClassifier\n",
    "\n",
    "#################################################################################################################################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import neighbors,preprocessing, cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "####################### Data load, preprocessing, One Hot Encoding of Categorical Data, Normalization of Numerical data ############################\n",
    "\n",
    "# In this section of code , I got the count of unique values in capital-gain, capital-loss, and native-country Using value_counts(), \n",
    "# 91.2% of capital-gain consists of 0 \n",
    "# 91.22% of native-country column consists of United States\n",
    "# 95.29% of capital-loss consists of 0.\n",
    "# Education number is mapped to Education column do Education column is redundant.\n",
    "# These columns don't contribute to my salary predictions so I decided to drop these columns  \n",
    "# Many of the machine learning algorithms can not operate on Categorical Data so I used One Hot encoding to convert the categorical \n",
    "# data to numerical data [2].\n",
    "# Label encoding is not useful in our case as there are a lot of unique categories in a column and it assumes the highe the categorical \n",
    "# value, better the category.\n",
    "# For example, if I have categories A, B, and C labelled with 1,2,and 3 respectively. Lable encoding assumes 3+1/2 = 2 which means \n",
    "# Average of A and C is B. This would definitely be a disaster in my salary predictions.\n",
    "# That is the reason why I chose to apply one hot encoding to categorical data. It binarize the each category and assumes it as a \n",
    "# feature to train the model.It uses pandas get_dummies to perform binarization as shown in oneHOtEncoder() method .\n",
    "# Normalization is required when features of the dataset have different ranges of numerical data [9]. So I used MinMaxScaler of sklearn\n",
    "# preprocessing to convert the data of numeric features to a common scale (0,1). NormalizeNumeircals method will take numerical column\n",
    "# and scale it [9].\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"salary\"]\n",
    "dataframe = pd.read_csv('dataset1_processed.csv',names = columns)\n",
    "\n",
    "def dropRedundantColumns():\n",
    "    \n",
    "    dataframe[\"capital-loss\"].value_counts()              #use print to check the output\n",
    "    dataframe[\"native-country\"].value_counts()\n",
    "    dataframe[\"capital-gain\"].value_counts()\n",
    "    dataframe.drop('capital-gain', axis=1, inplace=True)\n",
    "    dataframe.drop('capital-loss', axis=1, inplace=True)\n",
    "    dataframe.drop('education', axis=1,inplace=True)\n",
    "    dataframe.drop('native-country',axis=1,inplace=True)\n",
    "    \n",
    "dropRedundantColumns()\n",
    "\n",
    "\n",
    "def oneHotEncoder(columnName):\n",
    "    \n",
    "    newDfTrain = pd.concat([dataframe, pd.get_dummies(dataframe[columnName],prefix=columnName,prefix_sep='_')], axis=1)\n",
    "    newDfTrain.drop(columnName,axis=1,inplace=True)\n",
    "    return newDfTrain\n",
    "      \n",
    "dataframe = oneHotEncoder(\"workclass\")     \n",
    "dataframe = oneHotEncoder(\"marital-status\")\n",
    "dataframe = oneHotEncoder(\"occupation\")    \n",
    "dataframe = oneHotEncoder(\"relationship\")  \n",
    "dataframe = oneHotEncoder(\"race\")          \n",
    "dataframe = oneHotEncoder(\"sex\")\n",
    "dataframe = oneHotEncoder(\"education-num\")\n",
    "\n",
    "dataframe['salary'] = dataframe['salary'].apply(lambda salaryData: 1 if salaryData=='>50K' else 0)\n",
    "\n",
    "def normalizeNumericals(columnName):\n",
    "    \n",
    "    dataframe[columnName] = dataframe[columnName].astype(float)\n",
    "    scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    columnScaled = scale.fit_transform(dataframe[columnName].values.reshape(-1, 1))\n",
    "    dataframe[columnName] = pd.DataFrame(columnScaled)\n",
    "    \n",
    "normalizeNumericals('age')\n",
    "normalizeNumericals('fnlwgt')\n",
    "normalizeNumericals('hours-per-week')\n",
    "\n",
    "\n",
    "\n",
    "################################################ Data Split #####################################################################\n",
    "\n",
    "# For splitting the data into train and validation sets , I used cross validation method of sklearn. It will split the dataset as\n",
    "# 70% train data and 30% validation data as I set the test size to 0.3 [8].\n",
    "# y variables will be my target data and X variables will be my independent variables.\n",
    "# labels are useful in labelling confusion matrix.\n",
    "\n",
    "##################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "data = np.array(dataframe.drop(['salary'], 1))\n",
    "target = np.array(dataframe['salary']) \n",
    "\n",
    "X_train, X_val, y_train, y_val = cross_validation.train_test_split(data, target, test_size=0.3)\n",
    "labels = [0, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################### Training of Logistic Regression, K-Nearest Neighbors, and Decision Tree algorithms ############################\n",
    "\n",
    "# I used Logistic Regression, K-Nearest Neighbors, and Decision Tree algorithms for training. \n",
    "# Logistic regression is a parametric model whereas KNN and Deicision tree are Non Parametric Model. I implemented classifyDatasets(),\n",
    "# calculateAverageClassAccuracy(),and compareResults().\n",
    "# classifyDatasets() takes classifier as argument and it will use fit() method of sklearn to train the ML model and score() function\n",
    "# to find the acuuracy of classifier [8].\n",
    "# Logistic regression takes hyperparameters like penalty, dual, and iterations, K-nearest neighbours takes n_neighbors hyperparameter\n",
    "# and decision tree takes max_depth,criterion of entroy or gini index as hyperparameter in our case. Hyperparameter are useful in finding the best accuracy scores for \n",
    "# a classifier [7].\n",
    "# predict() will perform the predictions and confusion_matrix will be useful for finding the true positive rate, true negative rate,\n",
    "# average class accuracy for both positive and negative class\n",
    "# From the confusion matrix statisctics like TP(True Positives), FP(False Positives), TN(True Negatives), FN(False Negatives) will\n",
    "# be available [6]. This information is useful in finding the per class accuracy and average class accuracy as shown in calculateAverageClassAccuracy()\n",
    "# compareResults() will then compare the accuracy scores for each algorithms used and visualizing using bar chart [4]. From the accuracy score and average accuracy scores\n",
    "# it is clearly seen that, K-nearest neighbors classifier has accuracy score of around 89% in both training sets and validation sets but \n",
    "# While considering average class accuracy KNN shows 10% of difference between avg class accuracies of Train set(74%) and Validation sets(84%).\n",
    "# Using Logistic Regression classifier, accuracy scores for train and validation sets are of around 83% but average accuracy scores for both the sets\n",
    "# reached to 74%.\n",
    "# Using Decision Tree Classifier, accuracy scores for both the sets are around 84% but average accuracy scores shows 4% difference\n",
    "\n",
    "############################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "def classifyDatasets(classifier,classifierName):\n",
    "    \n",
    "    classifierTrainset = classifier.fit(X_train, y_train)\n",
    "    accTrain = classifierTrainset.score(X_train, y_train) * 100\n",
    "    print('\\t \\t Accuracy of '+ classifierName +' on training set: ' + str(accTrain))\n",
    "    classifierValidationset = classifier.fit(X_val, y_val)\n",
    "    accVal = classifierValidationset.score(X_val, y_val) * 100\n",
    "    print('\\t \\t Accuracy of '+ classifierName +' on validation set: ' + str(accVal),'\\n')\n",
    "    y_train_pred = classifierTrainset.predict(X_train)\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred, labels)\n",
    "    y_val_pred = classifierValidationset.predict(X_val)\n",
    "    cm_val = confusion_matrix(y_val, y_val_pred, labels)\n",
    "    \n",
    "    return cm_train,cm_val,accTrain,accVal\n",
    "\n",
    "def calculateAverageClassAccuracy(cm_train,cm_val):\n",
    "    \n",
    "    print('\\t Trainset Statistics  : \\n')\n",
    "    \n",
    "    TP =  cm_train[0][0]\n",
    "    FP =  cm_train[0][1]\n",
    "    FN =  cm_train[1][0]\n",
    "    TN =  cm_train[1][1]\n",
    "    \n",
    "    print('\\t \\t True Positives in train set  :', TP)\n",
    "    print('\\t \\t False Positives in train set :', FP)\n",
    "    print('\\t \\t False Negatives in train set :', FN)\n",
    "    print('\\t \\t True Negatives in train set  :', TN)\n",
    "    \n",
    "    positiveAccuracy = TP*100 / (TP+FP)\n",
    "    negativeAccuracy = TN*100 / (TN+FN)\n",
    "    averageClassAccuracyTrain = (positiveAccuracy + negativeAccuracy) / 2\n",
    "    overallAccuracy = (TP + TN ) * 100 / (TP + FP + TN + FN)\n",
    "    print('\\t \\t Average Class Accuracy in Train Set : ', averageClassAccuracyTrain) \n",
    "    print('\\n \\n')\n",
    "    \n",
    "    print('\\t Validation Set Statistics  : \\n')\n",
    "    \n",
    "    TP =  cm_val[0][0]\n",
    "    FP =  cm_val[0][1]\n",
    "    FN =  cm_val[1][0]\n",
    "    TN =  cm_val[1][1]\n",
    "    \n",
    "    print('\\t \\t True Positives in validation set  :', TP)\n",
    "    print('\\t \\t False Positives in validation set :', FP)\n",
    "    print('\\t \\t False Negatives in validation set :', FN)\n",
    "    print('\\t \\t True Negatives in validation set  :', TN)\n",
    "        \n",
    "    positiveAccuracy = TP*100 / (TP+FP)\n",
    "    negativeAccuracy = TN*100 / (TN+FN)\n",
    "    averageClassAccuracyVal = (positiveAccuracy + negativeAccuracy) / 2\n",
    "    overallAccuracy = (TP + TN ) * 100 / (TP + FP + TN + FN)\n",
    "    print('\\t \\t Average Class Accuracy in Validation Set : ', averageClassAccuracyVal) \n",
    "    print('\\n \\n')\n",
    "    \n",
    "    return averageClassAccuracyTrain,averageClassAccuracyVal\n",
    "    \n",
    "\n",
    "def compareResults(classifiers,accuracies, setType, scoreName):\n",
    "    \n",
    "    index = np.arange(len(classifiers))\n",
    "    plot.bar(index, accuracies)\n",
    "    plot.xlabel('Classifiers', fontsize=12)\n",
    "    plot.ylabel(scoreName + ' Score for '+ setType +' Set', fontsize=12)\n",
    "    plot.xticks(index, classifiers, fontsize=10, rotation=30)\n",
    "    plot.title(scoreName + ' Score Comparison of Classifiers on '+setType+' Set')\n",
    "    plot.show()\n",
    "    \n",
    "# Logistic Regression classsifier   \n",
    "print('Logistic Regression Classifier : \\n')    \n",
    "logisticRegressionClf = LogisticRegression(penalty='l2',dual=False,max_iter=100)\n",
    "cm_train,cm_val,accTrainLog,accValLog = classifyDatasets(classifier=logisticRegressionClf,classifierName=\"Logistic Regression Classifier \")\n",
    "averageClassAccuracyTrainLog,averageClassAccuracyValLog = calculateAverageClassAccuracy(cm_train,cm_val) \n",
    "\n",
    "\n",
    "# Decision Tree Classifier\n",
    "print('Decision Tree Classifier : \\n')\n",
    "decisionTreeClf = DecisionTreeClassifier(criterion='entropy', max_depth=8)\n",
    "cm_train,cm_val,accTrainDec ,accValDec = classifyDatasets(classifier=decisionTreeClf,classifierName=\"Decision Tree Classifier\")\n",
    "averageClassAccuracyTrainDec,averageClassAccuracyValDec=calculateAverageClassAccuracy(cm_train,cm_val)  \n",
    "\n",
    "#  KNN classifier\n",
    "print('KNN Classifier : \\n')\n",
    "knnClf = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "cm_train,cm_val,accTrainKnn,accValKnn = classifyDatasets(classifier=knnClf,classifierName=\"K-Nearest Neighbors classifier\")\n",
    "averageClassAccuracyTrainKnn,averageClassAccuracyValKnn = calculateAverageClassAccuracy(cm_train,cm_val)  \n",
    "\n",
    "# Comparing the results of classifier on train and validation sets\n",
    "classifiersList = [\"Logistic Regression\", \"Decision Tree\",\"KNN Classifier\"]\n",
    "trainAccuracies = [accTrainLog,accTrainDec,accTrainKnn]\n",
    "valAccuracies = [accValLog,accValDec,accValKnn]\n",
    "trainAvgClsAcc = [averageClassAccuracyTrainLog,averageClassAccuracyTrainDec,averageClassAccuracyTrainKnn]\n",
    "valAvgClsAcc = [averageClassAccuracyValLog,averageClassAccuracyValDec,averageClassAccuracyValKnn]\n",
    "\n",
    "compareResults(classifiers=classifiersList,accuracies=trainAccuracies, setType=\"Train\",scoreName=\"Accuracy\")\n",
    "compareResults(classifiers=classifiersList,accuracies=valAccuracies, setType=\"Validation\",scoreName=\"Accuracy\")\n",
    "compareResults(classifiers=classifiersList,accuracies=trainAvgClsAcc, setType=\"Train\",scoreName=\"Avg Accuracy\")\n",
    "compareResults(classifiers=classifiersList,accuracies=valAvgClsAcc, setType=\"Validation\",scoreName=\"Avg Accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "############# Variation in validation set avg class accuracy according to maxdepth parameter of decision tree ############################\n",
    "\n",
    "# testingParameter() will check the average accuracy scores and accuracy scores using variation of max depth hyperparameter of\n",
    "# decision tree classifier in validation sets [1] [3].\n",
    "# It will calculate average class accuracy scores, overall accuracy scores for different values of max_depth and append to the \n",
    "# respective lists.\n",
    "# Line chart will take max_depth and accuracy scores as x and y labels respectively.Line chart will be useful in visualizing the \n",
    "# variation of max_depth parameter (Accuracy vs max_depth) [4] [5]\n",
    "\n",
    "#######################################################################################################################################\n",
    "\n",
    "avgAcc  = []\n",
    "depths = []\n",
    "accuracyKnn = []\n",
    "\n",
    "def testingParameter():\n",
    "    \n",
    "    for i in range(1,11):\n",
    "\n",
    "        decisionTreeClf = DecisionTreeClassifier(criterion='entropy', max_depth=i) \n",
    "        decisionTreeClfVal = decisionTreeClf.fit(X_val, y_val)\n",
    "        accVal = decisionTreeClfVal.score(X_val, y_val) * 100\n",
    "        y_val_pred = decisionTreeClfVal.predict(X_val)\n",
    "        cm_val = confusion_matrix(y_val, y_val_pred, labels)\n",
    "\n",
    "        TP =  cm_val[0][0]\n",
    "        FP =  cm_val[0][1]\n",
    "        FN =  cm_val[1][0]\n",
    "        TN =  cm_val[1][1]\n",
    "\n",
    "        positiveAccuracy = TP*100 / (TP+FP)\n",
    "        negativeAccuracy = TN*100 / (TN+FN)\n",
    "        averageClassAccuracyVal = (positiveAccuracy + negativeAccuracy) / 2\n",
    "\n",
    "        depths.append(i)\n",
    "        avgAcc.append(averageClassAccuracyVal)\n",
    "        accuracyKnn.append(accVal)\n",
    "        \n",
    "testingParameter()    \n",
    "\n",
    "def plotAvgclassAcc(depths,avgAcc):\n",
    "    \n",
    "    plot.plot(depths,avgAcc, color='g')\n",
    "    plot.xlabel('Maximum Depth ')\n",
    "    plot.ylabel('Average Class Accuracy')\n",
    "    plot.title('Decision Tree Classifier Variation in Avg Class Accuracy')\n",
    "    plot.show()\n",
    "\n",
    "def plotoverallAcc(depths,accuracyKnn):\n",
    "    \n",
    "    plot.plot(depths,accuracyKnn, color='g')\n",
    "    plot.xlabel('Maximum Depth ')\n",
    "    plot.ylabel('Accuracy')\n",
    "    plot.title('Decision Tree Classifier Variation in Accuracy')\n",
    "    plot.show()\n",
    "    \n",
    "    \n",
    "plotAvgclassAcc(depths,avgAcc)\n",
    "plotoverallAcc(depths,accuracyKnn)\n",
    "\n",
    "######################################## Parameter Tuning of Decision Tree Classifier ############################################\n",
    "\n",
    "# Reason behind choosing Decision Tree Algorithm is the fitting and predictions are faster than K-nearest neightbors and thr accuracy\n",
    "# scores are better than Logistic Regression algorithm. Moreover, K-nearest neighbors is lazy learner which take less time in \n",
    "# training but more time predicting whereas Decision Tree Algorithm which is eager learner hence it is faster in both the case [3].\n",
    "# Our ultimate goals is salary predication which makes Decision Tree Algorithm a better choice in our case. Decision tree is also\n",
    "# useful to pick up nonlinearities in data which makes it fairly accurate than others [3].\n",
    "# Criterion hyperparameter of decision tree can have Entropy and Gini index which are considered for the tuning in our case [3].\n",
    "# After visualizing the data and doing some research , Gini index takes a little less time in computation than entropy criterion\n",
    "# We are dealing with a lot of data ,so I decided to go with Gini Index.Using Gini index and setting max_depth to 9 helped me in\n",
    "# increasing the average class accuracy and accuracy scores to 86% and 76% in validations sets.Which are 3% more than I achieved\n",
    "# using entropy criterion and max_depth of 7.\n",
    "# decTreeUsingGini() calculates the accuracy scores for validation sets for different hyperparameter value (criterion='gini', max_depth=9).\n",
    "\n",
    "#######################################################################################################################################\n",
    "\n",
    "def decTreeUsingGini():\n",
    "    \n",
    "    decisionTreeClf = DecisionTreeClassifier(criterion='gini', max_depth=9)\n",
    "    decisionTreeClfVal = decisionTreeClf.fit(X_val, y_val)\n",
    "    accVal = decisionTreeClfVal.score(X_val, y_val) * 100\n",
    "    print('After Hyperparameter Tuning in Decision Tree : \\n ')\n",
    "    print('\\t \\t Accuracy of Decision Tree Classifier on validation set using gini index: ' + str(accVal),'\\n')\n",
    "    y_val_pred = decisionTreeClfVal.predict(X_val)\n",
    "    cm_val = confusion_matrix(y_val, y_val_pred, labels)\n",
    "    \n",
    "    TP =  cm_val[0][0]\n",
    "    FP =  cm_val[0][1]\n",
    "    FN =  cm_val[1][0]\n",
    "    TN =  cm_val[1][1]\n",
    " \n",
    "    positiveAccuracy = TP*100 / (TP+FP)\n",
    "    negativeAccuracy = TN*100 / (TN+FN)\n",
    "    averageClassAccuracyVal = (positiveAccuracy + negativeAccuracy) / 2\n",
    "    print('\\t \\t Average Class Accuracy of Decision Tree Classifier on validation set using gini index: ' + str(averageClassAccuracyVal),'\\n')\n",
    "        \n",
    "decTreeUsingGini()\n",
    "\n",
    "###################################################### Predictions ###############################################################\n",
    "\n",
    "# After tuning the classifier, it is time to do the predictions. First I converted dataset1_test.csv  into dataframe and then\n",
    "# I performed the normalization, one hot encoding to test dataframe. Then I converted it into numpy array start predicting the \n",
    "# salary for the test dataset usiing Decision Tree Classifier. It will append the results to B00805073_prediction.csv\n",
    "\n",
    "#################################################################################################################################### \n",
    "\n",
    "columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\"]\n",
    "dataframe_test = pd.read_csv('dataset1_test.csv',names = columns );\n",
    "\n",
    "dataframe_test.drop('capital-gain', axis=1, inplace=True)\n",
    "dataframe_test.drop('capital-loss', axis=1, inplace=True)\n",
    "dataframe_test.drop('education', axis=1,inplace=True)\n",
    "dataframe_test.drop('native-country',axis=1,inplace=True)\n",
    "\n",
    "def oneHotEncoderTest(columnName):\n",
    "    \n",
    "    newDfTest = pd.concat([dataframe_test, pd.get_dummies(dataframe_test[columnName],prefix=columnName,prefix_sep='_')], axis=1)\n",
    "    newDfTest.drop(columnName,axis=1,inplace=True)\n",
    "    return newDfTest\n",
    "\n",
    "dataframe_test = oneHotEncoderTest(\"workclass\")     \n",
    "dataframe_test = oneHotEncoderTest(\"marital-status\")\n",
    "dataframe_test = oneHotEncoderTest(\"occupation\")    \n",
    "dataframe_test = oneHotEncoderTest(\"relationship\")  \n",
    "dataframe_test = oneHotEncoderTest(\"race\")          \n",
    "dataframe_test = oneHotEncoderTest(\"sex\")           \n",
    "dataframe_test = oneHotEncoderTest(\"education-num\") \n",
    "\n",
    "def normalizeNumericalsTest(columnName):\n",
    "    \n",
    "    dataframe_test[columnName] = dataframe_test[columnName].astype(float)\n",
    "    scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    columnScaled = scale.fit_transform(dataframe_test[columnName].values.reshape(-1, 1))\n",
    "    dataframe_test[columnName] = pd.DataFrame(columnScaled)\n",
    "    \n",
    "normalizeNumericalsTest('age')\n",
    "normalizeNumericalsTest('fnlwgt')\n",
    "normalizeNumericalsTest('hours-per-week')\n",
    "\n",
    "X_test = np.array(dataframe_test)\n",
    "y_test_pred =  decisionTreeClf.predict(X_test)\n",
    "predictions = y_test_pred.tolist()\n",
    "\n",
    "def exportPredictions():\n",
    "    count = 0     \n",
    "    for i in range(len(predictions)):\n",
    "\n",
    "        if(predictions[i]==0):\n",
    "            count+=1\n",
    "            predictions[i] = '<=50K'\n",
    "\n",
    "        elif(predictions[i]==1):\n",
    "            predictions[i] = '>50K'\n",
    "    \n",
    "    print(count)\n",
    "    \n",
    "    df = pd.DataFrame(predictions)        \n",
    "    df.to_csv('B00805073_prediction.csv', header=False,index=False)\n",
    "\n",
    "exportPredictions()\n",
    "\n",
    "##################################################### References ########################################################################\n",
    "\n",
    "\n",
    "# [1]\"Decision Trees: How to Optimize My Decision-Making Process?\", Medium, 2019. [Online]. Available: https://medium.com/cracking-the-data-science-interview/decision-trees-how-to-optimize-my-decision-making-process-e1f327999c7a. [Accessed: 07- Jun- 2019].\n",
    "\n",
    "# [2]J. Brownlee, \"Why One-Hot Encode Data in Machine Learning?\", Machine Learning Mastery, 2019. [Online]. Available: https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/. [Accessed: 07- Jun- 2019].\n",
    "\n",
    "# [3]\"InDepth: Parameter tuning for Decision Tree\", Medium, 2019. [Online]. Available: https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3. [Accessed: 07- Jun- 2019].\n",
    "\n",
    "# [4]\"Data Visualization in Python — Line Graph in Matplotlib\", Medium, 2019. [Online]. Available: https://medium.com/@pknerd/data-visualization-in-python-line-graph-in-matplotlib-9dfd0016d180. [Accessed: 07- Jun- 2019].\n",
    "\n",
    "# [5]2019. [Online]. Available: https://medium.com/python-pandemonium/data-visualization-in-python-bar-graph-in-matplotlib-f1738602e9c4. [Accessed: 07- Jun- 2019].\n",
    "\n",
    "# [6]D. Gopinath, \"Confusion Matrix - Get Items FP/FN/TP/TN - Python\", Data Science Stack Exchange, 2019. [Online]. Available: https://datascience.stackexchange.com/questions/28493/confusion-matrix-get-items-fp-fn-tp-tn-python. [Accessed: 07- Jun- 2019].\n",
    "\n",
    "# [7]A. Zheng, \"Evaluating Machine Learning Models\", O'Reilly Media, 2019. [Online]. Available: https://www.oreilly.com/ideas/evaluating-machine-learning-models/page/3/evaluation-metrics. [Accessed: 07- Jun- 2019].\n",
    "\n",
    "# [8]\"Solving A Simple Classification Problem with Python — Fruits Lovers’ Edition\", Towards Data Science, 2019. [Online]. Available: https://towardsdatascience.com/solving-a-simple-classification-problem-with-python-fruits-lovers-edition-d20ab6b071d2. [Accessed: 07- Jun- 2019].\n",
    "\n",
    "# [9]M. Aquilina and B. Musa, \"Normalize columns of pandas data frame\", Stack Overflow, 2019. [Online]. Available: https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame. [Accessed: 07- Jun- 2019]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
